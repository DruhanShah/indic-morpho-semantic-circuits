#+title: Indic Morpho-Semantic Circuits
 #+latex_header: \author{Druhan Rajiv Shah \\ IIIT Hyderabad \And Sidharth K \\ IIIT Hyderabad \And Anshul Krishnadas Bhagwat \\ IIIT Hyderabad}

#+options: toc:nil num:nil 

#+latex_header: \usepackage{acl}

#+bibliography: ./custom.bib
#+cite_export: natbib apa


#+begin_abstract
Mechanistic interpretability methods have been used to obtain task-specific circuits in autoregressive language models that demonstrate understanding of syntactic and semantic information to an extent. However, such analyses have been focused on a single language, usually English, and the circuits obtained have reflected its structure. In this project, we explore circuits identifying semantic roles in small-scale models for Hindi and English and compare their complexity and their use of Hindi's morphological complexity and free word order. We find that the circuits differ quite strongly and reflect linguistic features of the respective languages, suggesting a difference in model learning across languages at a syntactic level, which has impacts on general conclusions drawn from mechanistic analyses.
#+end_abstract


* Introduction

The advent of large-scale generative language models has revolutionized natural language processing (NLP), yet their internal workings remain largely opaque. This "black box" problem is a significant barrier to building truly trustworthy and reliable AI systems. The field of Mechanistic Interpretability (MI) seeks to address this by reverse-engineering the specific, human-understandable algorithms that models learn during training [cite:@elhage2021mathematical].

However, a lot of MI research has been concentrated on English-language models, creating a critical gap in our understanding of how a Transformer-based model adapts to the typological diversity of human languages. Different languages encode grammatical relationships in fundamentally different ways. For instance, English relies heavily on a fixed Subject-Verb-Object (SVO) word order. In contrast, Hindi uses a more flexible but primarily Subject-Object-Verb (SOV) order [cite:@verma1970word], marking grammatical roles with distinct postpositional particles.

In this project, we explore how these typological differences shape the internal circuits of language models. We will train three identical, autoregressive models (\sim 124.5M parameters) from scratch, one each on English, Hindi, and Telugu. We will then use visualisations, probes, and causal interventions to identify and compare the neural circuits each model develops to process semantic roles.

By comparing the circuits that emerge in response to these distinct grammatical strategies, we can move beyond simply knowing that models work for different languages to understanding precisely how they adapt their computational mechanisms. This study will attempt to provide a mechanistic, comparative analysis of semantic role processing in generative models across different language families, offering foundational insights into the functioning of Transformer-based AI systems.


* Background and Related Work

SRL is a central process in NLP that seeks to uncover the roles that words (or their computational counterparts: tokens) perform as arguments to the sentence's core verb or predicate. The PropBank [cite:@10.1162/0891201053630264] schema provides a set of roles that each argument is classified into, including =ARG0= (the entity performing the action out of their own volition), =ARG1= (the entity on whom the action is performed), and =ARGM-LOC= (the verb modifier that indicates location) among others. While classifier models trained to classify tokens by semantic role are numerous, we aim to study generative models and their use of semantic roles instead.

The representation of semantic roles in each language is unique, with English relying on word order, and Hindi using separate postpositional particles (case markers) to indicate, but not necessarily determine semantic roles [cite:@vaidya-etal-2011-analysis].
The work by [cite/t:@ghosh.etal2024] relies on the well-established idea that languages which have no Dominant word order (like Hindi and to an extent Telugu) encode information about semantic roles in sentences with morphosyntactic features like /kāraka/ markers or /vibhaktī/ affixes [cite:@vaidya-etal-2011-analysis]. Consequently, autoregressive language models can be expected to use differing methods to encode and use information like semantic roles in order to generate tokens.

The field of Mechanistic Interpretability (MI) seeks to reverse-engineer the internal algorithms learned by transformer-based language models during training. This approach moves beyond performance metrics to explain the specific computational mechanisms underlying a model's behavior. The fundamental unit of analysis in MI is the transformer circuit: [cite:@elhage2021mathematical] a subgraph of model parameters and activations responsible for a discrete task. Research in this area has successfully identified key circuits, such as Induction Heads [cite:@olsson.etal2022], and circuits for indirect object identification [cite:@wang.etal2022], which are critical for in-context learning and recall. However, a significant portion of MI research has concentrated on English-language models. This focus creates a critical gap in understanding how a transformer's architecture adapts to the vast typological diversity of human languages. Grammatical relationships are encoded in fundamentally different ways across language families, and the circuits developed to process them are likely to differ accordingly.


* Experimental setup

** Model Training

We train GPT2-style transformer language models with ~18M parameters (6 layers and 6 heads per layer) for English and Hindi separately. The choice of models is appropriately small to still be able to learn the provided dataset and the corresponding linguistic structures, while still being small enough to run comfortably on available compute and be easily interpretable.

*** Dataset

We use the Tiny Stories dataset for the English corpus and a machine-translated variant of the same [fn::Obtained from =OmAlve/TinyStories-Hindi= on HuggingFace] for Hindi. This ensures similar semantic content and equal complexity of data, while simultaneously preventing performance issues caused due to data scarcity as described by [].

* 

The relevant repository for this project is [[https://github.com/DruhanShah/indic-morpho-semantic-circuits][on GitHub (this link)]].

#+print_bibliography: t
