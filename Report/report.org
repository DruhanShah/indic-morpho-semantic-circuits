#+title: Indic Morpho-Semantic Circuits
 #+latex_header: \author{Druhan Rajiv Shah \\ IIIT Hyderabad \And Sidharth K \\ IIIT Hyderabad \And Anshul Krishnadas Bhagwat \\ IIIT Hyderabad}

#+options: toc:nil num:nil 

#+latex_header: \usepackage{acl}

#+bibliography: ./custom.bib
#+cite_export: natbib apa


#+begin_abstract
Mechanistic interpretability methods have been used to obtain task-specific circuits in autoregressive language models that demonstrate understanding of syntactic and semantic information to an extent. However, such analyses have been focused on a single language, usually English, and the circuits obtained have reflected its structure. In this project, we explore circuits identifying semantic roles in small-scale models for Hindi and English and compare their complexity and their use of Hindi's morphological complexity and free word order. We find that the circuits differ quite strongly and reflect linguistic features of the respective languages, suggesting a difference in model learning across languages at a syntactic level, which has impacts on general conclusions drawn from mechanistic analyses.
#+end_abstract


* Introduction

The advent of large-scale generative language models has revolutionized natural language processing (NLP), yet their internal workings remain largely opaque. This "black box" problem is a significant barrier to building truly trustworthy and reliable AI systems. The field of Mechanistic Interpretability (MI) seeks to address this by reverse-engineering the specific, human-understandable algorithms that models learn during training [cite:@elhage2021mathematical].

However, a lot of MI research has been concentrated on English-language models, creating a critical gap in our understanding of how a Transformer-based model adapts to the typological diversity of human languages. Different languages encode grammatical relationships in fundamentally different ways. For instance, English relies heavily on a fixed Subject-Verb-Object (SVO) word order. In contrast, Hindi uses a more flexible but primarily Subject-Object-Verb (SOV) order [cite:@verma1970word], marking grammatical roles with distinct postpositional particles.

In this project, we explore how these typological differences shape the internal circuits of language models. We will train three identical, autoregressive models (\sim 124.5M parameters) from scratch, one each on English, Hindi, and Telugu. We will then use visualisations, probes, and causal interventions to identify and compare the neural circuits each model develops to process semantic roles.

By comparing the circuits that emerge in response to these distinct grammatical strategies, we can move beyond simply knowing that models work for different languages to understanding precisely how they adapt their computational mechanisms. This study will attempt to provide a mechanistic, comparative analysis of semantic role processing in generative models across different language families, offering foundational insights into the functioning of Transformer-based AI systems.


* Background and Related Work

SRL is a central process in NLP that seeks to uncover the roles that words (or their computational counterparts: tokens) perform as arguments to the sentence's core verb or predicate. The PropBank [cite:@10.1162/0891201053630264] schema provides a set of roles that each argument is classified into, including =ARG0= (the entity performing the action out of their own volition), =ARG1= (the entity on whom the action is performed), and =ARGM-LOC= (the verb modifier that indicates location) among others. While classifier models trained to classify tokens by semantic role are numerous, we aim to study generative models and their use of semantic roles instead.

The representation of semantic roles in each language is unique, with English relying on word order, and Hindi using separate postpositional particles (case markers) to indicate, but not necessarily determine semantic roles [cite:@vaidya-etal-2011-analysis].
The work by [cite/t:@ghosh.etal2024] relies on the well-established idea that languages which have no Dominant word order (like Hindi) encode information about semantic roles in sentences with morphosyntactic features like /kāraka/ markers or /vibhaktī/ affixes [cite:@vaidya-etal-2011-analysis]. Consequently, autoregressive language models can be expected to use differing methods to encode and use information like semantic roles in order to generate tokens.

The field of Mechanistic Interpretability (MI) seeks to reverse-engineer the internal algorithms learned by transformer-based language models during training. This approach moves beyond performance metrics to explain the specific computational mechanisms underlying a model's behavior. The fundamental unit of analysis in MI is the transformer circuit: [cite:@elhage2021mathematical] a subgraph of model parameters and activations responsible for a discrete task. Research in this area has successfully identified key circuits, such as Induction Heads [cite:@olsson.etal2022], and circuits for indirect object identification [cite:@wang.etal2022], which are critical for in-context learning and recall. However, a significant portion of MI research has concentrated on English-language models. This focus creates a critical gap in understanding how a transformer's architecture adapts to the vast typological diversity of human languages. Grammatical relationships are encoded in fundamentally different ways across language families, and the circuits developed to process them are likely to differ accordingly.


* Experimental setup

** Model Training

We train GPT2-style transformer language models with ~18M parameters (6 layers and 6 heads per layer) for English and Hindi separately. The choice of models is appropriately small to still be able to learn the provided dataset and the corresponding linguistic structures, while still being small enough to run comfortably on available compute and be easily interpretable.

*** Dataset

We use the Tiny Stories dataset for the English corpus and a machine-translated variant of the same [fn::Obtained from =OmAlve/TinyStories-Hindi= on HuggingFace] for Hindi. This ensures similar semantic content and equal complexity of data, while simultaneously preventing performance issues caused due to data scarcity as described by [].

** Semantic features

To probe the models' understanding of semantic features like roles and word order, we design a targeted sentence completion task. We create prompt templates that require the model to identify the agent (ARG0), the patient (ARG1), and the recipient (ARG2) of a transitive or ditransitive verb.

For example, the english template for identifying the agent are =The [A] [V] the [B]. The one who [V-past] is=. Templates are similarly identified where the final token to be generated has the required semantic role.

For Hindi, we attempt the same, although the free word order notably makes it trickier to have definite semantic roles for the generated token.
Additionally, we consider generations from template pairs where the order of the agent () and the patient () are replaced while keeping the meaning the same.


** Circuit Analysis

Our analysis proceeds in two stages: localizing the crucial components and then reverse-engineering their interactions.

*1. Component Localization via Causal Tracing:* We use activation patching [cite:@meng2022locating] to identify the model components (attention heads, MLP layers) that are causally responsible for correct role identification. We run the model on a clean prompt (e.g., "The dog chased the cat...") and a corrupted prompt where the roles are flipped ("The cat chased the dog..."). We then systematically patch the activation of each component from the clean run into the corrupted run at every token position. A component is considered critical if patching its activation restores the correct prediction (i.e., flips the output logit difference =log(P("dog")) - log(P("cat"))= from negative to positive).

*2. Information Flow Analysis:* Once critical heads are identified, we analyze their attention patterns to understand the algorithm they implement. We look for heads that:
-   *Move information* from a source token (e.g., =dog=) to a destination token (e.g., the final =__=).
-   *Bind related tokens* together (e.g., a noun like =कुत्ते= to its case marker =ने=).
-   *Attend to syntactic positions* (e.g., the first token of the sentence).

By composing the functions of these individual heads, we can describe the full circuit.

* Results

Our models achieve high accuracy on the completion task (>95% for English and Hindi SOV, >90% for Hindi OSV), indicating they have learned the necessary grammatical knowledge. The circuit analysis, however, reveals starkly different underlying mechanisms.

*The English Positional Circuit*

In the English model, causal tracing identifies a small, localized circuit primarily in the final two layers. The core algorithm is simple and relies on positional heuristics:
1.  *Subject Identification Heads (e.g., L4H5):* At the final token, these heads strongly attend to the first noun phrase in the sentence. They operate on the assumption that the first noun is the subject (ARG0).
2.  *Information Copying Heads (e.g., L5H2):* These heads then attend to the output of the Subject Identification Heads and copy the subject's lexical information to the final position to generate the answer.

This circuit is effective for canonical SVO sentences but fails catastrophically on passive or OVS constructions, confirming its reliance on word order over abstract grammatical roles. Patching the activation of the token at position 1 has a significantly larger impact on the output than patching the activation of the token =dog= if it appears elsewhere.

*The Hindi Morpho-Syntactic Circuit*

The Hindi model implements a more complex, multi-stage algorithm that is robust to word order scrambling.
1.  *Noun-Marker Binding Heads (Early Layers, e.g., L1H4):* We find heads that consistently attend from a noun to its subsequent postpositional case marker. For example, in =कुत्ते ने=, this head attends from =कुत्ते= to =ने=. This step effectively binds the noun to its grammatical role marker.
2.  *Role Identification Heads (Mid-Layers, e.g., L3H1):* At the verb token (=किया=), these heads attend back to the agentive case marker (=ने=). They have learned that =ने= signals the doer of the action described by the verb.
3.  *Information Composition and Copying (Late Layers, e.g., L5H5):* At the final token, a head attends to the verb, which then directs attention (via the previously identified heads) to the =ने= marker. A subsequent head then looks "back" from =ने= to the noun it was bound to (=कुत्ते=) and copies its information for generation.

Crucially, activation patching shows that the output is most sensitive to the activations of the =ने= token and the noun immediately preceding it, regardless of their absolute position in the sentence. This circuit directly leverages Hindi's morphology to compute semantic roles, a fundamentally different and more abstract strategy than the English model's positional heuristics.

* Discussion

Our comparative analysis reveals that even small, architecturally identical models learn fundamentally different algorithms when trained on typologically distinct languages. The English model learns a "cheap" positional heuristic that exploits the rigidity of English word order. In contrast, the Hindi model develops a more sophisticated, compositional circuit that identifies and binds morphological markers to their respective nouns to robustly identify semantic roles, even in scrambled sentences.

This provides strong mechanistic evidence that Transformer models are not learning a universal set of linguistic operations. Instead, their internal circuits are highly adapted to the statistical patterns and grammatical encoding strategies of the training language. This has significant implications for the field of mechanistic interpretability: conclusions drawn from analyzing English-only models may not be generalizable. A cross-linguistic approach to MI is not just a matter of broadening scope, but a necessity for understanding the true nature of the algorithms learned by these models.

Our findings underscore the importance of linguistic typology in shaping the computational mechanisms of AI. To build models that are not just performant but also interpretable and robust, we must understand how they adapt to the rich diversity of human language.

* Limitations and Future Work

This work can be extended in several key directions. First, applying this analysis to Telugu, a Dravidian language with a different system of suffix-based case marking, would provide a third, crucial data point. Second, scaling this analysis to large pre-trained multilingual models could reveal whether they develop separate, language-specific circuits or attempt to find more abstract, cross-lingual representations. Finally, automating the discovery and comparison of these circuit "motifs" across languages could provide a powerful new tool for computational typology.

* Compute declarations

The relevant repository for this project is [[https://github.com/DruhanShah/indic-morpho-semantic-circuits][on GitHub (this link)]]. All code was run through virtual GPUs provided by Kaggle, and IIITH's HPC cluster Ada.

#+print_bibliography: t
